{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3481e1f-dfb4-49a1-9b90-6c45e14547d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8aaa62-7684-49ab-9ad1-3f0a9ad8f380",
   "metadata": {},
   "source": [
    "# Load Qwen2-VL-7B-Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88a4b7-dc0e-4806-9fa3-db2331561144",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the processor\n",
    "min_pixels = 1000000\n",
    "max_pixels = 1000000\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n",
    ")\n",
    "\n",
    "STAGES = ['Wake', 'N1', 'N2', 'N3', 'REM']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d02e6c-eca9-48d9-a457-716de692b483",
   "metadata": {},
   "source": [
    "# Load images generated based on ConMIL interpretations and prediction sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096226f-cc3c-49d6-ac9f-a85df4758c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction set figures\n",
    "sample_path = 'sleepEDF'\n",
    "\n",
    "# Load ground truth and prediction set\n",
    "gt_path = os.path.join(sample_path, \"ground_truth.txt\")\n",
    "with open(gt_path, \"r\") as f:\n",
    "    gt_text = f.read().strip()\n",
    "    gt = STAGES.index(gt_text.split(\":\")[1].strip())\n",
    "\n",
    "prediction_set = [fname.replace(\".png\", \"\") for fname in os.listdir(sample_path) if fname.endswith(\".png\") and fname != \"org.png\"]\n",
    "pset_indices = [STAGES.index(code) for code in prediction_set]\n",
    "\n",
    "# Load images for predictions\n",
    "conmil_paths = [os.path.join(sample_path, f\"{STAGES[idx]}.png\") for idx in pset_indices]\n",
    "images = [Image.open(path).convert(\"RGB\") for path in conmil_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41014f6-b09e-4558-9540-d477367a8537",
   "metadata": {},
   "source": [
    "# Generate instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c5639-994b-42a8-874f-8f1163cdfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dynamic instruction\n",
    "diagnosis_options = \", \".join([STAGES[idx] for idx in sorted(pset_indices)])\n",
    "instruction = (\n",
    "    f\"Given a Fpz-Cz EEG and visual interpretations for the following possible sleep stages: {diagnosis_options}, \"\n",
    "    f\"determine the most likely sleep stage. Use the provided model interpretations as reference and \"\n",
    "    f\"base your decision solely on these visual features without additional analysis or introducing new criteria.\\n\\n\"\n",
    "    f\"Provide your answer in the following format:\\n\"\n",
    "    f\"Conclusion: <Selected Sleep Stage>\\n\"\n",
    "    f\"Reason: <Brief reason for the choice based on visual features>\"\n",
    ")\n",
    "\n",
    "# Prepare messages for the processor\n",
    "content = [{\"type\": f\"Model interpretation for sleep stage: {STAGES[idx]}\", \"image\": img} for idx, img in zip(pset_indices, images)]\n",
    "content.append({\"type\": \"Instruction\", \"text\": instruction})\n",
    "messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "# Process vision info and prepare inputs\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=[image_inputs],\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")  # Move inputs to GPU if using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92de38d-e252-4886-964c-eb2da7c4ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Parse LLM response\n",
    "response = output_text[0].strip()\n",
    "diagnosis = response.split(\"Conclusion: \")[1].split(\"\\n\")[0].strip() if \"Conclusion: \" in response else \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f3ab7-ab41-4c38-97b8-4c9be8e61bfe",
   "metadata": {},
   "source": [
    "# Show LLM response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20304033-3deb-468d-8b8d-75e442f70726",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Instruction:\", instruction)\n",
    "print(\"Generated Response:\", response)\n",
    "print(\"Extracted Conclusion:\", diagnosis)\n",
    "print(\"Ground Truth:\", STAGES[gt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3750f-7068-48b9-a9ec-a7a62df2e873",
   "metadata": {},
   "source": [
    "# Now we see how LLM works without ConMIL's support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb31529-61f1-4034-9b3a-ec4f0325a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original image\n",
    "org_image_path = os.path.join(sample_path, \"org.png\")\n",
    "org_image = Image.open(org_image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c8f4d-8b10-4061-82a5-1eeb789b3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instruction\n",
    "instruction = (\n",
    "    f\"Given a Fpz-Cz EEG, determine the most likely sleep stage among the following categories:\"\n",
    "    f\"Wake, N1, N2, N3, or REM.\"\n",
    "    f\"Base your decision solely on the visual features of the provided EEG without performing additional analysis or introducing new criteria.\\n\\n\"\n",
    "    f\"Provide your answer in the following format:\\n\"\n",
    "    f\"Conclusion: <Selected Sleep Stage>\\n\"\n",
    "    f\"Reason: <Brief reason for the choice based on visual features>\"\n",
    ")\n",
    "\n",
    "# Prepare messages for the processor\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"EEG plot\", \"image\": org_image},\n",
    "            {\"type\": \"Instruction\", \"text\": instruction},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process vision info and prepare inputs\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=[image_inputs],\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")  # Move inputs to GPU if using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39951344-57b0-43fc-8dc9-1b03df66a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Parse LLM response\n",
    "response = output_text[0].strip()\n",
    "diagnosis = response.split(\"Conclusion: \")[1].split(\"\\n\")[0].strip() if \"Conclusion: \" in response else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4422de0e-523c-4323-bf85-3b0aa7d8a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Instruction:\", instruction)\n",
    "print(\"Generated Response:\", response)\n",
    "print(\"Extracted Diagnosis:\", diagnosis)\n",
    "print(\"Ground Truth:\", STAGES[gt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5045fc8-bdcb-4b58-8129-2ccb9b6fb688",
   "metadata": {},
   "source": [
    "# We can also see how LLM works with only ConMIL prediction set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817e3bc-5915-4194-b49c-8c8083c6925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instruction\n",
    "instruction = (\n",
    "    f\"Given a Fpz-Cz EEG, determine the most likely sleep stage among the following categories:\"\n",
    "    f\"{diagnosis_options}\"\n",
    "    f\"Base your decision solely on the visual features of the provided EEG without performing additional analysis or introducing new criteria.\\n\\n\"\n",
    "    f\"Provide your answer in the following format:\\n\"\n",
    "    f\"Conclusion: <Selected Sleep Stage>\\n\"\n",
    "    f\"Reason: <Brief reason for the choice based on visual features>\"\n",
    ")\n",
    "\n",
    "# Prepare messages for the processor\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"EEG plot\", \"image\": org_image},\n",
    "            {\"type\": \"Instruction\", \"text\": instruction},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process vision info and prepare inputs\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=[image_inputs],\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")  # Move inputs to GPU if using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d63d0e-c4c2-45f1-ab35-94baac0c2be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Parse LLM response\n",
    "response = output_text[0].strip()\n",
    "diagnosis = response.split(\"Conclusion: \")[1].split(\"\\n\")[0].strip() if \"Conclusion: \" in response else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810c532-b224-47ef-9208-6c4d9528a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Instruction:\", instruction)\n",
    "print(\"Generated Response:\", response)\n",
    "print(\"Extracted Diagnosis:\", diagnosis)\n",
    "print(\"Ground Truth:\", STAGES[gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359539a-3236-4eca-9393-8ef48f482860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b3ebc-5f94-4c5c-b6b0-52bf356f7980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
